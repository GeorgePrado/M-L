\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{bigints}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

\title{Introducci\'on a la Estad\'istica y Probabilidades CM-274}
\date{}
\maketitle


\vspace{0.5cm}


{\Large C\'alculo}

\vspace{0.3cm}


{\Large {\'Algebra Lineal }}

\vspace{0.3cm}


\textbf{Matrices y Vectores}

\begin{eqnarray*}
A=\begin{bmatrix}
a_{11} & a_{12}\ldots & a_{1m}\\
a_{21} & \ddots&\ddots & a_{2m}\\
\vdots & \ddots & \ddots&\vdots\\
a_{n1} & \ldots & \ldots & a_{nm}
\end{bmatrix} = [a_{ij}]_{n\times m}
\end{eqnarray*}

\vspace{0.3cm}

\begin{eqnarray*}
v=\begin{bmatrix}
v_1\\
v_2\\
\vdots\\
v_n
\end{bmatrix}
\end{eqnarray*}


\vspace{0.3cm}

\begin{itemize}
	\item {Suma}: $A+B=[a_{ij}+b_{ij}]_{n\times m}$ donde $A=[a_ij]_{n\times m}$ y $B=[b_ij]_{n\times m}$.
	\item {Producto}: $AB=\left[\displaystyle\sum_{k=1}^{p}a_{ik}b_{kj}\right]_{n\times m}$ donde $A=[a_ij]_{n\times p}$ y $B=[b_ij]_{p\times m}$.\\
Este producto solo est\'a definido si el n\'umero de columnas de $A$ es el n\'umero de filas de $B$.
	\item {Producto}: $cA=\left[ca_{ij}\right]_{n\times m}$ donde $A=[a_ij]_{n\times p}$ y $c\in \mathbb{R}$.
\end{itemize}
\begin{itemize}
	\item {Asociatividad}: $(AB)C=A(BC)$
	\item {Distributividad}: $A(B+C)=AB+AC$
	\item {No Conmutatividad}: Si $A=[a_{ij}]_{n\times p}$ y $B=[b_{ij}]_{p\times m}$, luego $AB=[c_{ij}]_{n\times m}$; sin embargo, $BA$ no est\'a definido si $n\neq m$. En caso $m=n$, entonces $AB=[c_{ij}]_{n\times n}$ y $BA=[c_{ij}]_{p\times p}$ pero no necesariamente $n=p$, es decir $AB\neq BA$ en general.
\end{itemize}


\vspace{0.3cm}


\begin{itemize}
	\item { Matriz cuadrada}: Sea $A\in\mathbb{R}^{m\times n}$, diremos que $A$ es una matriz cuadrada si $m=n$.
	\item {Matriz Diagonal} Sea $A\in\mathbb{R}^{n\times n}$, diremos que $A=[a_{ij}]_{n\times n}$ es una matriz diagonal si
	$a_{ij}=0$ para todo $i\neq j$.
	\item { Matriz Identidad}: La matriz identidad $I\in \mathbb{R}^{n\times n}$, donde $I=[a_{ij}]_{n\times n}$,  se define como una matriz diagonal tal que $a_{ij}=1$ para todo $i=j$.
	\item { Matriz transpuesta}: Sea $A\in\mathbb{R}^{n\times n}$, donde $A=[a_{ij}]_{m\times n}$, definimos su transpuesta como $A^T=[a_{ji}]_{n\times m}$.
	\item {Matriz sim\'etrica}: Sea $A\in\mathbb{R} ^{m\times n}$, diremos que $A$ es una matriz sim\'etrica si $A=A^T$.
	\item { Matriz Inversa}: Sea $A\in\mathbb{R}^{n\times n}$, diremos que $A$ tiene inversa si existe una matriz $B\in\mathbb{R}^{n\times n}$ tal que $AB=I$. L matriz inversa se denota como $A^{-1}$.\\
Se cumple la conmutatividad $AA^{-1}=A^{-1}A=I$). 
	\item { Matriz Ortogonal.}: Sea $A\in\mathbb{R}^{n\times n}(n\geq2)$, es matriz ortogonal si $A^{-1}=A^T$.
\end{itemize}

\vspace{0.3cm}


 Sea $A\in\mathbb{R}^{n\times n}$, se define la traza de $A$ como

\vspace{0.3cm}

\begin{eqnarray*}
	Tr(A) = \sum_{i=1}^{n}a_{ii}, & \;\mbox{ donde } A=[a_{ij}]_{n\times n}.
	\end{eqnarray*}
	
\vspace{0.5cm}


Propiedades de la Traza:

\begin{itemize}
	\item Sea $A,B\in\mathbb{R}^{n\times m}$, entonces $Tr(A+B)=Tr(A)+Tr(B)$
	\item Sea $A\in\mathbb{R}^{n\times m}$ y $c\in\mathbb{R}$, entonces $cTr(A)=Tr(cA)$
	\item Sea $A\in\mathbb{R}^{n\times m}$ y $B\in\\mathbb{R}^{m\times n}$, entonces $Tr(AB)=Tr(BA)$.
\end{itemize}

\vspace{0.3cm}

	Si $A\in\mathbb{R}^{n\times m}$, luego $AA^{T}$ y $A^{T}A$ son matrices cuadradas sim\'etricas.

\vspace{0.3cm}


	Dado que $A\in\mathbb{R}^{n\times m}$, tenemos que $A^T\in\mathbb{R}^{m\times n}$ entonces $AA^T\in\mathbb{R}^{n\times n}$ y $AA^T\in\mathbb{R}^{m\times m}$, matrices cuadradas.

\vspace{0.2cm}

	Además $(AA^T)^T=(A^T)^TA^T=AA^T$ por tanto $AA^T$ es sim\'etrica; an\'alogamente $A^TA$ es sim\'etrica.

Producto escalar Se define el producto escaalr de $u,v\in\mathbb{R}^n$ como
	\begin{eqnarray*}
	u\cdot v=u^Tv=\sum_{i=1}^{n}u_iv_i,\;\;\;\mbox{ donde }u=[u_i]_{n\times 1},v=[v_i]_{n\times 1}.
	\end{eqnarray*}

Propiedades del Producto Escalar:
\begin{itemize}
	\item {Definida positiva}: $(v\cdot v)\geq 0$. Adem\'as $v\cdot v = 0$ si y solo si $v=0$.
	\item {Simetr\'ia}: $u\cdot v=v\cdot u$.
	\item {Linealidad}: $(au+bv)\cdot w=a(u\cdot w)+b(v\cdot w)$.
\end{itemize}
Se denota la norma de un vector $v\in \mathbb{R}^n$ por $\vert v \vert $ y se define como
	\begin{eqnarray*}
	\vert v\vert=\sqrt{v\cdot v}=\sqrt{\sum_{i=1}^{n}v_i^2},\;\;\;\mbox{ donde }v=[v_i]_{n\times 1},
	\end{eqnarray*}
	representando su longitud.
	
	\vspace{0.3cm}
	
Se define la distancia entre $u,v\in\mathbb{R}^n$ como
	\begin{eqnarray*}
	d(u,v)=\vert u-v\vert
	\end{eqnarray*}

 \vspace{0.5cm}

Sean $u,v\in\\mathbb{R}^n$, se dice que $u$ y $v$ son ortogonales si $u\cdot v=0$. Adem''as, un conjunto $\{u_1,u_2,\ldots,u_m\}\subset\mathbb{R}^n$ $(m\leq n)$ es un conjunto ortogonal si $u_i\cdot u_j=0$ para todo $i\neq j$.

\vspace{0.3cm}

	Un vector $v\in\mathbb{R}^n$ es normal, o unitario, si $\vert v \vert =1$.
\vspace{0.3cm}

 Un conjunto $\{u_1,u_2,\ldots,u_m\}\subset\mathbb{R}^n$ $(m\leq n)$ es ortonormal si es un conjunto ortogonal y además $\vert u_i\vert =1$ para todo $i=1,\ldots,n$.

\vspace{0.3cm}

Sea $A\in \mathbb{R}^{n\times n}$, si sus vectores columna $u_1,\ldots,u_n$ forman un conjunto ortonormal, entonces $A$ es ortogonal.

\vspace{0.2cm}

Veamos que $A$ es ortogonal, es decir que $A^{-1}=A^T$

\vspace{0.3cm}

\begin{eqnarray*}
	A^TA=\begin{bmatrix}
	u_1^T\\
	\vdots\\
	u_n^T
	\end{bmatrix}\begin{bmatrix}
	u_1 & u_2 & \ldots & u_n
	\end{bmatrix} = \begin{bmatrix}
	1 & \ldots & 0\\
	\vdots & \ddots & \vdots\\
	0 & \ldots & 1
	\end{bmatrix} = \mathbb{I}_{n\times n}
	\end{eqnarray*}
	
	lo \'ultimo es a causa de que $u_i^Tu_i =1$ y $u_i^Tu_j =0$ si $i\neq j$.
	
	\vspace{0.3cm}
	
 Un conjunto $\{u_1,u_2,\ldots,u_m\}\subset\mathbb{R}^n$ $(m\leq n)$ es linealmente independiente si la siguiente igualdad

\vspace{0.2cm}

	\begin{eqnarray*}
	\sum_{i=1}^{m}c_i u_i = 0 & \mbox{ con } c_i\in\mathbb{R} \mbox{ para todo }i=1,\ldots,m
	\end{eqnarray*}
	solo se satisface cuando $c_i=0$ para todo $i=1,\ldots,m$.

\vspace{0.2cm}

Sea $A\in\mathbb{R}^{n\times m}$, se define el rango de $A$, denotado como el n\'umero de columnas de $A$ linealmente independientes.
	
\vspace{0.5cm}

\textbf{Autovectores y Autovalores}

\vspace{0.3cm}

 Sean $A\in\mathbb{R}^{n\times n}$ y $v\in\mathbb{R}^n$ distinto de cero, se dice que $v$ es autovector de $A$ si
	
	\vspace{0.3cm}
	
	\begin{eqnarray*}
	Av=\lambda v, & \mbox{ para alg\'un } \lambda\in\mathbb{R}.
	\end{eqnarray*}
	Se dice que $\lambda$ es el autovalor de $A$, correspondiente a $v$.

\vspace{0.3cm}

	Si $v$ es un autovector, con valor propio $\lambda\neq 0$, de $A^{T}A$, luego $Av$ es un autovector con el mismo valor propio $\lambda$ de $AA^{T}$.

\vspace{0.3cm}

	Sea $v$ es un autovector de $A^{T}A$ y $\lambda\neq 0$ su autovalor correspondiente, entonces $A^{T}Av=\lambda v$.\\
	Luego multiplicando por $A$ a la izquierda
	$$\begin{array}{cccc}
	&A(A^{T}Av)&=&A(\lambda v)\\
	\Rightarrow& AA^{T}(Av)&=&\lambda (Av).
	\end{array}$$
	 Por lo que $Av$ es autovector de $AA^T$ y $\lambda\neq0$ su autovector correspondiente.

\vspace{0.3cm}

	Sea $A\in\mathbb{R}^{n\times n}$, entonces $A^TA$ no tiene autovalores negativos.

\vspace{0.3cm}


	Sea $u$ un autovector de $A^TA$ y $\lambda$ su autovalor correspondiente, entonces $A^TAu=\lambda u$, luego
	$$\begin{array}{rcl}
	u^T(A^TAu)&=&u^T(\lambda u)\\
	(u^TA^T)Au&=&\lambda u^Tu\\
	(Au)^TAu&=&\lambda \vert u\vert\\
	\vert Au\vert ^2&=&\lambda \vert u\vert^2\\
	0\leq \dfrac{\vert Au\vert^2}{\vert u\vert^2}&=&\lambda\\
	\end{array}$$

\vspace{0.5cm}

\textbf{Valores Singulares}
	Sea $A\in\mathbb{R}^{n\times m}$, se definen los valores singulares de $A$ como las ra\'ices cuadradas de los eigenvalores de $A^TA$.

\vspace{0.5cm}

Sea $A\in\mathbb{R}^{n\times n}$, se dice que $A$ es diagonalizable si y solo si $A$ puede ser descompuesta como

\vspace{0.3cm}

	\begin{eqnarray*}
	A = UDU^{-1} & \mbox{ donde } U = \begin{bmatrix}
	u_1 &\ldots & u_n
	\end{bmatrix} \mbox{ y }D=diag(\lambda_1,\ldots,\lambda_n).
	\end{eqnarray*}
	donde $u_1,\ldots,u_n$ son autovectores de $A$, asociados a los autovectores $\lambda_1,\ldots,\lambda_n\in\mathbb{R}$.
	Se dice que $A$ es ortogonalmente diagonalizable si $A = UDU^{T}$.	

\vspace{0.3cm}

	Sea la matriz $A\in\mathbb{R}^{n\times n}$ con autovalores $\lambda_1,\lambda_2,\ldots,\lambda_n\in\mathbb{R}$ entonces

\vspace{0.2cm}

\begin{eqnarray*}
	\sum_{i=1}^{n}\lambda_i = Tr(A).
	\end{eqnarray*}

\vspace{0.5cm}

	Como $A=UDU^{-1}$, usando las propiedades de la traza tenemos que

\vspace{0.2cm}


	\begin{eqnarray*}
	Tr(A)=Tr(UDU^{-1})=Tr(U^{-1}UD)=Tr(D)=\sum_{i=1}^{n}\lambda_i
	\end{eqnarray*}

\vspace{0.3cm}


\textbf{Teorema Espectral}

\vspace{0.2cm}

	Si $A\in\mathbb{R}^{n\times n}$ es sim\'etrica, entonces existe una base ortonormal  formada por autovectores de $A$.

\vspace{0.3cm}

	Sea $A\in\mathbb{R}^{n\times n}$ sim\'etrica, entonces es ortogonalmente diagonalizable.
	
\vspace{0.5cm}
	
Sea $v\in\mathbb{R}^n$ y $P$ un subespacio de $\mathbb{R}^n$, se define la proyecci\'on ortogonal de $v$ sobre $P$ como $w=proy_{\;P}(v)\in S$  tal que

	\begin{eqnarray*}
	\vert w-v\vert \leq \vert z-v\vert  & \mbox{ para todo }z\in S.
	\end{eqnarray*}

\vspace{0.3cm}

\textbf{Matriz de proyecci\'on}

\vspace{0.3cm}

	Dado $\{u_1,u_2,\ldots,u_m\}$ base de un subespacio $S\subset\mathbb{R}^n$ y sea $ v\in \mathbb{R}^n$, entonces la proyecci\'on ortogonal de $v$ sobre $S$ es
	
	\vspace{0.3cm}
	
	\begin{eqnarray*}
	proy_{\;S}(v)= U(U^TU)^{-1}U^Tv & \mbox{ donde }U=\begin{bmatrix}
	u_1 & \ldots & u_m.
	\end{bmatrix}
	\end{eqnarray*}
	
	\vspace{0.2cm}
	
	$P=U(U^TU)^{-1}U^T$ se conoce como la matriz de proyecci\'on.
	
	\vspace{0.3cm}
	
\textbf{Descomposici\'on Ortogonal} 

Dado $\{u_1,u_2,\ldots,u_m\}$ base ortogonal de un subespacio $S\subset\mathbb{R}^n$ y sea $ v\in \mathbb{R}^n$, entonces la proyecci\'on ortogonal de $v$ sobre $S$ es

\vspace{0.3cm}

	\begin{eqnarray*}
	\displaystyle {proy}_{\;S}(v)=\dfrac{v\cdot u_1}{\vert u_1\vert ^2}u_1+\dfrac{v\cdot u_2}{\vert u_2\vert ^2}u_2+\ldots + \dfrac{v\cdot u_m}{\vert u_m\vert ^2}u_m.
	\end{eqnarray*}
	
\vspace{0.3cm}

Si $\{u_1,u_2,\ldots,u_m\}$ es una base ortonormal entonces:
\vspace{0.3cm}


\begin{eqnarray*}
\displaystyle {proy}_{\;S}(v)=(v\cdot u_1)u_1+(v\cdot u_2)u_2+\ldots + (v\cdot u_m)u_m
\end{eqnarray*}

\vspace{0.3cm}

	Por propiedad  $proy_{\;S}(v)=U(U^TU)^{-1}U^Tv$, donde $U=\begin{bmatrix}
	u_1 &\ldots & u_m
	\end{bmatrix}$. Dado que el conjunto $\{u_1,u_2,\ldots,u_m\}$ es ortogonal entonces $u_i^Tu_j=0$ para todo $i\neq j$, luego
	\begin{eqnarray*}
	(U^TU)^{-1} = \left(\begin{bmatrix}
	u_1^T\\
	\vdots\\
	u_m^T
	\end{bmatrix}\begin{bmatrix}
	u_1 & \ldots & u_m
	\end{bmatrix}\right)^{\!\!\!\!\!-1} = \left(\begin{bmatrix}
	 \vert u_1\vert ^2 & \ldots &0\\
	 \vdots & \ddots & \vdots\\
	 0 & \ldots & \vert u_m\vert ^2
	\end{bmatrix}\right)^{\!\!\!\!\!-1} = \begin{bmatrix}
	\dfrac{1}{\vert u_1\vert ^2} & \ldots &0\\
	\vdots & \ddots & \vdots\\
	0 & \ldots & \dfrac{1}{\vert u_m\vert ^2}
	\end{bmatrix}
	\end{eqnarray*}
	Luego,
	\begin{eqnarray*}
	proy_{\;S}(v) &= & \begin{bmatrix}
	u_1 & \ldots & u_m
	\end{bmatrix}\begin{bmatrix}
	\dfrac{1}{\vert u_1\vert ^2} & \ldots &0\\
	\vdots & \ddots & \vdots\\
 	0 & \ldots & \dfrac{1}{\vert u_m\vert ^2}
	\end{bmatrix}\begin{bmatrix}
	u_1^T\\
	\vdots\\
	u_m^T\\
	\end{bmatrix}v
	=  \begin{bmatrix}
	u_1 & \ldots & u_m
	\end{bmatrix}\begin{bmatrix}
	\dfrac{u_1^Tv}{\vert u_1\vert ^2} \\
	\vdots\\
	\dfrac{u_m^Tv}{\vert u_m\vert ^2}
	\end{bmatrix}\nonumber
	\end{eqnarray*}
	Por tanto $\displaystyle {proy}_{\;S}(v)=\sum_{i=1}^{m}\dfrac{v\cdot u_i}{\vert u_i\vert ^2}u_i$. Si $\{u_1,\ldots,u_m\}$ es un conjunto ortogonal entonces $\vert u_i\vert=1$, por lo que $\displaystyle {proy}_{\;S}(v)=\sum_{i=1}^{m}(v\cdot u_i)u_i$.
	
	
	\vspace{0.3cm}


\textbf{Ley de Variaci\'on de los Componentes}

\vspace{0.3cm}

	Dado $\{u_1,u_2,\ldots,u_n\}$ una base ortonormal de $\mathbb{R}^n$  y sea $p\in\mathbb{R}^n$. Si los vectores unitarios $e_i$, del sistema de coordenadas, se desplazan hacia los vectores $u_i$ respectivamente, formando un nuevo sistema, entonces $p$ en este nuevo sistema de coordenadas viene dado por

\vspace{0.3cm}


	\begin{eqnarray*}
	p_2 = Up & \mbox{ donde }U=\begin{bmatrix}
	u_1 & \ldots & u_n
	\end{bmatrix} \mbox{ se denomina matriz de rotaci\'on.}
	\end{eqnarray*}

\vspace{0.3cm}

Ahora si $S$ es el subespacio generado por $\{u_1,\ldots,u_m\}$, extendiendola  a una  base ortonormal de $R^n$, sea $\{u_1,\ldots,u_n\}$ entonces la proyecci\'on de $v\in\mathbb{R}^n$ sobre S, por propiedad , se tiene que 
	
	\vspace{0.3cm}
	
	\begin{eqnarray*}
	\displaystyle {proy}_{\;S}(v)&=&(v\cdot u_1)u_1+\ldots + (v\cdot u_n)u_m + 0u_{m+1}\ldots + 0u_{n}\\
	 & = &\begin{bmatrix}
	 u_1&\ldots &u_m & u_{m+1} &\ldots &u_n
	 \end{bmatrix}\begin{bmatrix}
	 u_1^Tv \\
	 \vdots\\
	 u_m^Tv \\
	 0\\
	 \vdots\\
	 0
	 \end{bmatrix}
	\end{eqnarray*}
	
	
	Por esta propiedad el vector $(u_1^Tv,\ldots,u_m^Tv,0,\ldots,0)\in\mathbb{R}^n$ representa el vector $proy_{\;S}(v)$ en otro sistema de coordenadas y se puede  incorporarse en un sistema de menor dimensi\'on
	
	\vspace{0.3cm}
	
	\begin{eqnarray*}
	proy_{\; S}(v)\in\mathbb{R}^n\equiv (u_1^Tv,\ldots,u_m^Tv,0,\ldots,0)\in\mathbb{R}^n \equiv (u_1^Tv,\ldots,u_m^Tv)\in\mathbb{R}^m  
	\end{eqnarray*}
	
	\vspace{0.3cm}
	
	Como la  $proy_{\;S}(v)$ puede ser representado en un espacio $\mathbb{R}^{m}$ como $U^Tv$ donde $U=\begin{bmatrix}
	u_1&\ldots&u_m
	\end{bmatrix}$, se tiene la siguiente propiedad
	
	\vspace{0.3cm}
	
	
		Sea $S$ subespacio de $\mathbb{R}^n$ con una base ortonormal $u_1,\ldots,u_m$ que la genere y sea $v\in\mathbb{R}^n$, entonces $proy_{\;S}(v)$ puede ser incorporado en $\mathbb{R}^n$, mediante una rotaci\'on de su sistema de coordenadas, como el vector
		
		\vspace{0.2cm}
		
		\begin{eqnarray*}
		\begin{bmatrix}
		u_1^T\\
		\vdots\\
		u_m^T
		\end{bmatrix}v
		\end{eqnarray*}.
\end{document}