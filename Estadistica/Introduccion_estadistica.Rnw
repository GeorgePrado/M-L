\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath} 
\usepackage{mathtools}
\usepackage[spanish]{babel}
\usepackage{latexsym}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
library(animation)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@

\title{Introducci\'on a la Estad\'istica y Probabilidades CM-274}
\date{}
\maketitle

{\Large Introducci\'on a la Estad\'istica}



\vspace{0.8cm}

En Estad\'istica los datos obtenidos sirven  para obtener  los par\'ametros y las distribuciones de un \mbox{determinado} modelo y realizar  predicciones. 

\vspace{0.5cm}

\textbf{Poblaci\'on, muestras, par\'ametros y estad\'isticos}

\vspace{0.3cm}

Una \texttt{poblaci\'on} agrupa  todo las unidades de inter\'es. Una caracter\'istica n\'umericas de la poblaci\'on es un \texttt{par\'ametro}. Una \texttt{muestra} consiste de unidades que se recogen  de la poblaci\'on,  usada para hacer predicciones  acerca de la poblaci\'on. Alguna funci\'on de una muestra se llama \textbf{estad\'istico}.


En la pr\'actica recogemos  los datos en forma de muestras aleatorias de la poblaci\'on; esos son nuestros datos a los cuales podemos  medirlos, hacer c\'alculos y estimar los par\'ametros deconocidos de la poblaci\'on bajo cierta medida de precisi\'on.

\vspace{0.5cm}

\textbf{Notaci\'on:}

\vspace{0.2cm}

\begin{itemize}
\item $\theta$ = par\'ametro poblacional.
\item $\hat{\theta}$ = estimador, obtenido de la muestra.
\end{itemize} 

\vspace{0.5cm}

\textbf{Ejemplo 1 }   Incluso si el $80 \%$ de los usuarios est\'a conforme  con su conexi\'on a internet, esto no significa que exactamente $8$ de cada $10$ usuarios en la muestra observada est\'an conforme con el servicio. Por ejemplo con una probabilidad de $0.0328$, s\'olo la mitad de los diez consumidores est\'a conforme . En otras palabras, hay un $3\%$ de probabilidad  para que una muestra aleatoria  sugerir que contrariamente  al par\'ametro de la poblaci\'on, no m\'as que el $50\%$ de usuarios est\'an satisfechos.

\vspace{0.8cm}

\textbf{Errores de muestreo y no muestreo}

\vspace{0.3cm}

Los errores muestrales y no muestrales se refieren a la diferencia entre las muestras recogidas y la poblaci\'on.

\begin{itemize}
\item \texttt{Errores de muestreo} es causado por el hecho de que una muestra o porci\'on de la poblaci\'on es recogida. Cuando el tama\~no de la muestra crece, para muchos procedimientos estad\'isticos el error de muestreo decae.
\item \texttt{Errores de no muestreo} son causados por un inapropiado esquema de muestras o errores en las t\'ecnicas estad\'sticas. 
\end{itemize}

\vspace{0.5cm}


\textbf{Ejemplo 2 } La comparando  dos marcas de port\'atiles, un alto directivo pide a todos los empleados de su grupo establecer que port\'atil, les gusta y generaliza las respuestas obtenidas a la conclusi\'on, de que  port\'atil es mejor. Una vez m\'as, estos empleados no son  seleccionados  aleatoriamente  de la poblaci\'on de todos los usuarios que usan  las port\'atiles. Adem\'as, es probable que sean dependientes de sus opiniones, trabajando en conjunto, estas personas a menudo se comunican, y sus puntos de vista se afectan entre s\'i. Observaciones dependientes  no necesariamente causan errores de no muestreo, si se saben manejar  adecuadamente. El hecho es que, en tales casos, no podemos asumir  independencia.

\vspace{0.3cm}

\textbf{Ejemplo 3 }  Una popular revista semanal predijo correctamente os ganadores de las elecciones \mbox{presidenciales} en los a\~nos $1920, 1924, 1928$ y $1932$. Sin embargo fall\'o en $1936$, pese a tener   una muestra de $10$ millones de personas. Este es un cl\'asico ejemplo de errores de no muestreo.

\vspace{0.5cm}

\textbf{Muestreo Aleatorio Simple} es un dise\~no de muestreo donde los datos (unidades) se recogen de toda la poblaci\'on independiente unos  de otros, todos igualmente probable de ser incluidos en la muestra.


\vspace{0.6cm}

\textbf{Ejemplo 4}  Para evaluar la 'satisfacci\'on' de sus usuarios, un banco, hace una lista de todas sus \mbox{cuentas}. El \textbf{M\'etodo de Montecarlo} es usado para escoger un n\'umero aleatorio entre $1$ y $N$, donde $N$ es el n\'umero total de cuentas del banco, generando  una distribuci\'on uniforme en $(0,N)$ de la variable X y una muestra  del n\'umero de cuenta $\lceil{X}\rceil$ de la lista. Similarmente, escogemos una segunda cuenta, \mbox{distribuida uniformemente} entre las $N -1$ cuentas, etc, hasta conseguir una muestra de tama\~no $n$. Esto es una muestra aleatoria simple.


\vspace{0.3cm}

Obtener, un muestreo aleatorio representativo, es importante en estad\'istica. Aunque s\'olo una \mbox{porci\'on} de la poblaci\'on est\'a en nuestras manos, un riguroso dise\~no de  muestreo seguido por una adecuada \mbox{inferencia}  nos permite estimar par\'ametros y hacer predicciones con cierto grado de de confiabilidad.


\vspace{0.8cm}

\textbf{Estad\'isticos descriptivos simples}

\vspace{0.3cm}

Supongamos que tenemos el siguiente  muestreo aleatoria recogida 

\vspace{0.3cm}

$S = (X_1, X_2,\dots , X_n)$

\vspace{0.3cm}

Por ejemplo, para evaluar la efectividad de un procesador, para un cierto tipo de tareas, recogemos el tiempo del CPU de manera aleatoria para determinados trabajos  (en segundos) con  $n = 30$:  

\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c c}
70 & 36 & 43 & 69 & 82& 48 & 34 & 62 & 35 & 15 \\ 
59 & 139 & 46 & 37 & 42 & 30 & 55 & 56 & 36 & 82 \\ 
38 & 89 & 54 & 25 & 35& 24 & 22 & 9 & 56 & 19 \\ 
\end{tabular}
\end{table}

\vspace{0.3cm}

Qu\'e informaci\'on, conseguimos de estos  de n\'umeros?

\vspace{0.2cm}

Sabemos que X, el tiempo del CPU, es una variable aleatoria, y esos valores no tienen que estar entre los $30$ observados, pero estos datos pueden ser \'utiles para describir la distribuci\'on X.

Los estad\'isticos descriptivos simples miden la localizaci\'on, dispersi\'on, variabilidad y otras caracter\'isticas que pueden ser calculadas inmediatamente.  Por ejemplo tenemos:

\begin{enumerate}
\item \texttt{media} mide el promedio de los valores de una muestra.
\item \texttt{mediana} mide el valor central.
\item \texttt{cuantiles} o \texttt{cuartiles}, muestran donde ciertas partes de la muestra son localizadas.
\item \texttt{varianza, desviaci\'on est\'andar} y \texttt{rango intercuartil}, miden la variabilidad y dispersi\'on de los datos.
\end{enumerate} 


\vspace{0.3cm}

Cada estad\'istico es una variable aleatoria, ya que se calcula a partir de datos  aleatorios. Estos tienen una  \texttt{distribuci\'on muestral} que permite hacer inferencia.

Cada estad\'istico calcula el par\'ametro de la poblaci\'on correspondiente y a\~nade cierta informaci\'on acerca de la distribuci\'on de X, la variable de inter\'es.

\vspace{0.8cm}

\newpage 

\textbf{Media}

\vspace{0.3cm}

La media muestral $\overline{X}$ estima la media poblacional $\mu = \mathbb{E}(X)$.

\vspace{0.2cm}

Se define la media muestral $\overline{X}$ como el promedio aritm\'etico de 

\vspace{0.2cm}

\[
\overline{X} = \dfrac{X_1 + X_2 + \cdots X_n}{n}
\]

\vspace{0.2cm}

Naturalmente, siendo el promedio de observaciones en la muestra, $\overline{X}$ calcula el valor promedio  de toda la distribuci\'on de X. Para grandes muestras  recogidas se espera que converga a $\mu$.

Si la poblaci\'on tiene media y varianza finita, la media posee las siguientes propiedades:

\vspace{0.5cm}


Un estimador $\hat{\theta}$ es insesgado (\texttt{unbiased}) para un par\'ametro $\theta$ si su esperanza es igual al par\'ametro:

\[
\mathbb{E}(\hat{\theta}) = \theta
\]

para todos los posibles valores de $\theta$.

\vspace{0.2cm}


El sesgo (\texttt{Bias}) de $\hat{\theta}$ es definido como $Bias(\hat{\theta}) = \mathbb{E}(\hat{\theta} - \theta)$.


\vspace{0.3cm}

El insesgamiento(\texttt{unbiasedness}) significa que en un largo plazo,  recoger un gran n\'umero de muestras y el c\'alculo de $\hat{\theta}$, en promedio  nos permite alcanzar el  par\'ametro $\theta$  desconocido. En otras palabras, en un largo plazo el estimador insesgado no subestima ni sobreestima al par\'ametro $\theta$. La media muestral estima $\mu$ sin sesgo, ya que:


\vspace{0.5cm}


\[
\mathbb{E}(\overline{X}) = \mathbb{E}\Bigl(\dfrac{X_1 + \dots + X_n}{n} \Bigr) = \dfrac{\mathbb{E}X_1 + \dots \mathbb{E}X_n}{n} = \dfrac{n\mu}{n} = \mu.
\]


\vspace{0.5cm}

Un estimador $\hat{\theta}$ es consistente para un par\'ametro $\theta$ si el error muestral de alguna magnitud converge a $0$ cuando la muestra aumenta de tama\~no hacia el infinito. Es decir:

\[
\mathbb{P}\Bigl\{ \vert \hat{\theta} - \theta\vert > \epsilon \Bigr\} \rightarrow 0 \ \ \ \text{si}\ \ \  n \rightarrow \infty
\]

\vspace{0.2cm}

para un $\epsilon  > 0$.  


\vspace{0.3cm}

Para usar esta desigualdad, encontremos la varianza de $\overline{X}$,

\[
\mathbb{V}(\overline{X}) = \mathbb{V}\Bigl(\dfrac{X_1 + \dots + X_n}{n} \Bigr) = \dfrac{\mathbb{V}X_1 + \dots + \mathbb{V}X_n}{n^2} = \dfrac{n\sigma^2}{n^2} = \dfrac{\sigma^2}{n}.
\]

\vspace{0.3cm}

Por la desigualdad de Chebyshev para la variable aleatoria $\overline{X}$ tenemos que 

\[
\mathbb{P}\Bigl\{ \vert\overline{X} -\mu   \vert > \epsilon\Bigr\} \leq \dfrac{\mathbb{V}(\overline{X})}{\epsilon^2} = \dfrac{\sigma^2/n}{\epsilon^2} \rightarrow 0\ \ \ \text{cuando} \ \ \ n \rightarrow \infty.
\]

As\'i la media muestral es consistente.


\vspace{0.5cm}

\textbf{Normalidad as\'intotica}

\vspace{0.3cm}

Por el teorema del l\'imite central, la suma de observaciones y por tanto la media muestral tiene aproximadamente una distribuci\'on normal si son calculados desde una muestra muy grande.

Esto es, la distribuci\'on de 

\[
Z = \dfrac{\overline{X} - \mathbb{E}\overline{X}}{Sd\overline{X}} = \dfrac{\overline{X} - \mu}{\sigma\sqrt{n}}
\]

\vspace{0.3cm}

converge a la normal est\'andar cuando $n \rightarrow \infty$. Esta propiedad es llamada \texttt{normalidad as\'intotica}.


\vspace{0.5cm}

\textbf{Notaci\'on}

\begin{itemize}
\item $\mu$ = media poblacional
\item $\overline{X}$ = media muestral, estimador de $\mu$.
\end{itemize}

\vspace{0.2cm}

\begin{itemize}
\item $\sigma$ = desviaci\'on est\'andar poblacional.
\item $s$  = desviaci\'on est\'andar muestral, estimador de $\sigma$
\end{itemize}

\vspace{0.2cm}

\begin{itemize}
\item $\sigma^2$ = varianza  poblacional.
\item $s^2$  = varianza muestral, estimador de $\sigma^2$
\end{itemize}

\vspace{0.8cm}

\textbf{Mediana}

\vspace{0.3cm}

Una desventaja de la media muestral es su \texttt{sensitividad} a observaciones extremas. Otro medida de localizaci\'on es la \texttt{mediana muestral}, que estima la mediana poblacional y que es mucho menos sensitiva que la media muestral.

La mediana muestral $\overline{M}$  es un n\'umero que es excedido a lo m\'as por  la mitad de observaciones y es precedido por a lo m\'as por la mitad de observaciones.

\vspace{0.2cm}

La mediana poblacional $M$ es un n\'umero que es excedido con probabilidad no mayor que 0.5 y es precedido con probabilidad no mayor que 0.5, esto es

\[
\begin{dcases}
\mathbb{P}\{X > M\} \leq 0.5  \\
\mathbb{P}\{X < M\} \leq 0.5
\end{dcases}
\]

\vspace{0.3cm}



Comparando la media $\mu$ y la mediana, podemos decir si la distribuci\'on de X es sesgada a la derecha, sesgada a la izquierda o es sim\'etrica, como indica la figura:

\vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{g1.png}
\end{figure}

\vspace{0.3cm}

\begin{itemize}
\item Distribuci\'on sim\'etrica $\Rightarrow  M = \mu$.
\item Distribuci\'on sesgada a la derecha $\Rightarrow  M < \mu$.
\item  Distribuci\'on sesgada a la izquiera $\Rightarrow  M > \mu$.
\end{itemize}

\vspace{0.5cm}


Para distribuciones continuas, calcular la mediana poblacional se reduce a resolver una ecuaci\'on:


\[
\begin{dcases}
\mathbb{P}\{X > M\} = 1 - F(M) \leq 0.5  \\
\mathbb{P}\{X < M\}  = F (M)\leq 0.5
\end{dcases} \Rightarrow F(M) = 0.5
\]


\vspace{0.3cm}

Por ejemplo para la distribuci\'on exponencial con par\'ametro $\lambda$ que tiene un cdf

\[
F(x) = 1 - e^{-\lambda x}\ \ \ \text{para}\ \ \  x > 0.
\]


\vspace{0.2cm}

Resolver la ecuaci\'on $F(M) = 1 - e^{-\lambda x} = 0.5$ produce $M = \dfrac{\ln 2}{\lambda}$ y como $\mu = \dfrac{1}{\lambda}$, tenemos que $M < \mu$, lo que implica que la distribuci\'on exponencial es sesgada a la derecha.

\vspace{0.5cm}


Para distribuciones discretas, la ecuaci\'on $F(x) = 0.5$ tiene ya sea todo un intervalo de raices o no tiene raices. En el primer caso, alg\'un n\'umero en este intervalo excluyendo los extremos es una mediana (no es \'unica). Por ejemplo sea la distribuci\'on binomial con $n = 5$ y $p = 0.5$, entonces para todo $2 <x < 3$ , tenemos

\[
\begin{dcases}
\mathbb{P}\{X < x \} = F(2) =  0.5  \\
\mathbb{P}\{X > x\}  = 1 - F(2)=  0.5
\end{dcases}
\]

\vspace{0.2cm}

Por definici\'on, alg\'un intervalo $(2,3)$ es una mediana, como se muestra en el primer gr\'afico de la figura

\vspace{0.3cm}


Para una distribuci\'on binomial con $n = 5$ y $p = 0.4$, se tiene que $F(x) < 0.5$ si $x < 2$ y $F(x) > 0.5 $ para $x \geq 2$, pero no hay un valor $x$ tal que $F(x) = 0.5$. Entonces $M = 2$ es la mediana, como se muestra en el segundo gr\'afico de la figura.
 
 
 \vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{g2.png}
\end{figure}

\vspace{0.3cm}

Una muestra es siempre discreta, pues consiste de un n\'umero finito de observaciones. Entonces, el c\'alculo de la mediana muestral es similar al caso de distribuciones discretas.

En el muestreo aleatorio simple, todas las observaciones son igualmente probables, y por lo tanto, las probabilidades iguales  en cada lado de la mediana se traducen en un n\'umero igual de observaciones. Una vez m\'as, existen dos casos, dependiendo del tama\~no de la muestra $n$.

\begin{itemize}
\item Si $n$ es impar la $\Bigl(\dfrac{n + 1}{2} \Bigr)$ -\'esima  m\'as peque\~na observaci\'on es la mediana.
\item Si $n$ es par, el n\'umero entre las observaciones $\Bigl(\dfrac{n}{2} \Bigr)$-\'esima  m\'as peque\~na y la $\Bigl(\dfrac{n + 2}{2} \Bigr)$-\'esima m\'as peque\~na es la mediana.
\end{itemize}

\vspace{0.6cm}


\textbf{Ejemplo 5} Calculamos la mediana  con  $n = 30$ los tiempo del CPU determinados de manera aleatoria para determinados trabajos (en segundos) ordenados:


\vspace{0.3cm}


\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c c}
9 &15 &19 & 22&  24 &25& 30& 34& 35 &35 \\
36 & 36& 37& 38& 42& 43& 46& 48& 54& 55 \\
56 & 56 &59& 62 &69& 70& 82& 82 &89 &139

\end{tabular}
\end{table}

Desde que $n = 30$ es par, encontramos la $n/2 = 15$-\'esima m\'as peque\~na y $(n +2)/2$-\'esima m\'as peque\~na observaciones que son $42$ y $43$. Alg\'un n\'umero entre ellos es la mediana.

\vspace{0.8cm}

\textbf{Cuantil, percentiles y cuartiles}

\vspace{0.3cm}

\begin{itemize}
\item Un \texttt{p-cuantil} de una poblaci\'on es un n\'umero $x$ que resuelve la ecuaci\'on, para alg\'un $0 < p < 1$:

\vspace{0.2cm}


\[
\begin{dcases}
\mathbb{P}\{X < x \}  \leq p  \\
\mathbb{P}\{X > x\}  \leq 1 -p
\end{dcases} 
\]

 
 \item Un \texttt{p-cuantil} muestral es un n\'umero que excede a lo m\'as al $100p\%$ de la muestra y es excedido a lo m\'as por $100(1 -p)\%$, de la muestra.
 
 \item Un $\gamma$-\texttt{percentil} es $(0.01\gamma)$-cuantil.
 
 \item El primero, segundo, tercer cuartil son los $25, 50, 75$ percentiles. Ellos dividen una poblaci\'on o una muestra en cuatro partes iguales.
 \end{itemize}
 
 \vspace{0.3cm}
 
 Una mediana es al mismo tiempo el $0.5$-quantil, $50$ percentil y el segundo cuartil.
 
 
 \vspace{0.5cm}

\textbf{Notaci\'on}

\begin{itemize}
\item $q_p$ = \texttt{p-cuantil} poblacional
\item $\hat{q}_p$ = \texttt{p-cuantil} muestral, estimador de $q_p$.
\end{itemize}

\vspace{0.2cm}

\begin{itemize}
\item $\pi_{\gamma}$ = $\gamma$-percentil poblacional.
\item $\hat{\pi}_{\gamma}$  = $\gamma$-percentil  muestral, estimador de $\pi_{\gamma}$.
\end{itemize}

\vspace{0.2cm}

\begin{itemize}
\item $Q_1, Q_2, Q_3$ = cuartiles poblacional.
\item $\hat{Q}_1, \hat{Q}_2, \hat{Q}_3$ = cuartiles muestrales, estimadores de $Q_1, Q_2$ y $Q_3$
\end{itemize}
\vspace{0.2cm}

\begin{itemize}
\item $M$ = mediana poblacional
\item $\hat{M}$ = mediana muestral, estimador $M$.
\end{itemize}

\vspace{0.3cm}

Cuantiles, quartiles y percentiles est\'an relacionados como sigue:

\begin{itemize}
\item $q_p = \pi_{100p}$
\item $Q_1 = \pi_{25} = q_{1/4}$\ \ $Q_3 = \pi_{75} = q_{3/4}$
\item $M = Q_2 = \pi_{50} = q_{1/2}$
\end{itemize}


\vspace{0.5cm}


\textbf{Ejemplo 6} Calculemos el primer y el 3 quartil de los tiempos del CPU, del ejemplo 5:

\vspace{0.3cm}


\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c c}
9 &15 &19 & 22&  24 &25& 30& 34& 35 &35 \\
36 & 36& 37& 38& 42& 43& 46& 48& 54& 55 \\
56 & 56 &59& 62 &69& 70& 82& 82 &89 &139

\end{tabular}
\end{table}

\vspace{0.3cm}

\begin{itemize}
\item Para el primer cuartil $\hat{Q}_{1}$, tenemos que $p = 0.25$, encontramos que el $25\%$ de la muestra es igual $np = 7.5$ y el $75\%$ de la muestra es $n(1 -p) = 22.5$ observaciones. Desde la muestra ordenada, el octavo elemento $34$ no tiene m\'as que $7.5$ observaciones a la izquierda y no m\'as que $22.5$ oservaciones a la derecha. As\'i $\hat{Q}_{1} = 34$.
\item  Para el tercer cuartil $\hat{Q}_{3}$ el 23 elemento de la muestra  $\hat{Q}_{3} = 59$.
\end{itemize}


\vspace{0.8cm}

\textbf{Varianza y desviaci\'on est\'andar}

\vspace{0.5cm}

Para una muestra $(X_1, X_2, \dots, X_n)$ la \texttt{varianza muestral} es definida como 

\vspace{0.3cm}

\[
s^2 = \dfrac{1}{n -1}\displaystyle\sum_{i =1}^{n}(X_i - \overline{X})^2
\]

\vspace{0.3cm}

y mide la variabilidad entre las observaciones y estima la \texttt{varianza de la poblaci\'on} $\sigma^2 = \mathbb{V}(X)$.

\vspace{0.3cm}

La \texttt{desviaci\'on est\'andar muestral} es la raiz cuadrada de la varianza muestral.

\[
s = \sqrt{s^2}
\]

\vspace{0.3cm}

y se usa para medir  la variabilidad en las mismas unidades que X y calcula la desviaci\'on est\'andar de la poblaci\'on $\sigma$ = sd(X). 

La f\'ormula para $s^2$ se sigue de la misma idea $\sigma^2$, es tambi\'en el promedio de la desviaci\'on desde la media al cuadrado, calculada como una muestra. Como $\sigma^2$, la varianza muestral mide cuan lejos los valores de $X$ est\'an de su promedio.

\vspace{0.3cm}

Una manera de calcular la varianza muestral es usar la siguiente f\'ormula:

\vspace{0.2cm}

\[
s^2 = \dfrac{\displaystyle\sum_{i = 1}^{n}X_i^2 - n\overline{X}^2}{n - 1}.
\]


\vspace{0.3cm}

\textbf{Ejemplo 7} Para los datos del \texttt{Ejemplo 6} tenemos que $\overline{X}$ se sigue de la definici\'on o de la f\'ormula anterior que la varianza muestral es $s^2 = 703.1506 (\text{sec}^2)$.

\vspace{0.2cm}

La desviaci\'on est\'andar muestral es $s = \sqrt{703.1506} = 26.1506 (\text{sec})$. Como $\overline{X}$ y $s$ estiman la media poblacional y la desviaci\'on est\'andar, por la desigualdad de Chebishev podemos derivar que al menos $8/9$ de todas las tareas requiere menos que

\[
\overline{X} + 3s = 127.78 \text{sec}
\]

\vspace{0.3cm}

de tiempo del CPU.


\vspace{0.5cm}


El coeficiente $\dfrac{1}{n -1}$ asegura que $s^2$ es un estimador insesgado  de $\sigma^2$.

\vspace{0.3cm}

Para probar esto supongamos que $\mu = \mathbb{E}(X) = 0$, entonces $\mathbb{E}X_i^2 = \mathbb{V}X_i = \sigma^2$, as\'i 

\[
\mathbb{E}\overline{X} = \mathbb{V}\overline{X} = \sigma^2/n
\]

\vspace{0.2cm}

Entonces 

\[
\mathbb{E}s^2 = \dfrac{\mathbb{E}\displaystyle\sum X_i^2 - n\mathbb{E}\overline{X}^2}{n -1} = \dfrac{n\sigma^2 - \sigma^2}{n - 1} = \sigma^2.
\]

\vspace{0.3cm}

Si $\mu \neq 0$, consideramos la variable auxiliar $Y_i = X_i - \mu$ como la varianza no depende de ninguna constante $Y_i$ y $X$ tienen la misma varianza, lo que lleva a que sus varianzas son iguales tambi\'en:

\vspace{0.2cm}

\[
s_{Y}^2 = \dfrac{\displaystyle\sum(Y_i - \overline{Y})^2}{n - 1} = \dfrac{\displaystyle\sum(X_i + \mu -(\overline{X} - \mu))^2}{n - 1} = \dfrac{\displaystyle\sum(X_i - \overline{X})^2}{n -1} = s_{X}^2
\]

\vspace{0.2cm}

Desde que $\mathbb{E}(Y_i) = 0$, tenemos $\mathbb{E}(s_{X}^2) = \mathbb{E}(s_{Y}^2) = \sigma_{Y}^2 = \sigma_{X}^2$. De manera similar que para  $\overline{X}$ se puede mostrar que bajo ciertas condiciones, la varianza muestral y la desviaci\'on est\'andar muestral son consistentes y cumplen la propiedad de normalidad as\'intotica.


\vspace{0.8cm}

\textbf{Los errores est\'andar de las estimaciones}

\vspace{0.3cm}

El error est\'andar de un estimador $\hat{\theta}$ es la desviaci\'on est\'andar, $\sigma(\hat{\theta})$ = std$(\hat{\theta})$.


\vspace{0.2cm}

\begin{itemize}
\item $\sigma(\hat{\theta})$ = error est\'andar del estimador $\hat{\theta}$ de param\'etro $\theta$.
\item $s(\hat{\theta})$ = error est\'andar estimado = $\hat{\sigma}(\hat{\theta})$.
\end{itemize}
\vspace{0.3cm}

Como una medida de  variabilidad, los errores est\'andar demuestran precisi\'on y la fiabilidad de los estimadores. Muestran c\'omo  muchos estimadores del mismo  par\'ametro $\theta$ puede variar si se calculan a partir de diferentes muestras. Idealmente, nos gustar\'ia trabajar con estimadores insesgados o poco sesgados que tienen  un bajo error est\'andar.




\vspace{0.3cm}

 \textbf{Ejemplo 8} El error de est\'andar de la media muestral, se puede expresar como el param\'etro $\theta = \mu$ y es estimada desde la muestra de tama\~no $n$ por la media muestra $\hat{\theta} = \overline{X}$, como el error est\'andar de este estimador es $\sigma(\overline{X}) = \sigma / \sqrt{n}$ y puede ser estimado por $s(\overline{X}) = s/\sqrt{n}$.


\vspace{0.8cm}

\textbf{Rango intercuartil}

\vspace{0.3cm}

La media muestral, la varianza y la desviaci\'on est\'andar son sensibles a valores \texttt{at\'ipicos(outliers)}. Si una observaci\'on  at\'ipica  (outlier) aparece err\'oneamente en nuestro conjunto de datos,  puede afectar significativamente los valores de $\overline{X}$ y $s^2$.

En la pr\'actica, estos valores at\'ipicos  pueden ser un problema real que es dif\'icil de evitar. Para poder detectar e identificar valores at\'ipicos, necesitamos medidas de variabilidad que no son muy sensibles a ellos.

\vspace{0.2cm}

Una de estas medidas es un \texttt{rango intercuartil}, que es definido como la diferencia entre el primer y tercer cuartil 

\vspace{0.3cm}

\[
IQR = Q_3 - Q_1
\]

\vspace{0.2cm}

Esto mide la variabilidad de la data y es usado para detectar outliers. El IQR es estimado por el \texttt{rango intercuartil muestral}

\[
\widehat{IQR} = \hat{Q}_3 - \hat{Q}_1
\]

\vspace{0.3cm}

Una propiedad importante para  la identificaci\'on de los outliers es la regla de $1,5(IQR)$.
es decir la  medida $1.5(\hat{Q_3} - \hat{Q_1})$  desde el primer cuartil hasta el tercer cuartil. Todos los puntos de datos observados fuera de este rango  son los primeros candidatos a ser manejados como valores at\'ipicos.

\vspace{0.2cm}

La regla de $1.5(IQR)$ parte  de la suposici\'on de que los datos son casi son distribuido normalmente. Si esto es una suposici\'on v\'alida, entonces  el $99.3\%$ de la poblaci\'on
debe aparecer dentro del   $1.5$ rango intercuartil de los cuartiles. 


\vspace{0.5cm}

\textbf{Ejemplo 9} Calculemos $\widehat{IQR}$ con los datos sobre el tiempo del CPU en los ejemplos anteriores

\vspace{0.3cm}

\[
\widehat{IQR} = \hat{Q}_3 - \hat{Q}_1 = 59- 34 = 25
\]


\vspace{0.3cm}

y la medida del $1.5$ rango intercuartil para cada  cuartil:



\begin{align*}
\hat{Q}_1 -  1.5\widehat{IQR}  =  & 34 - 37.5  =  -3.5 \\
\hat{Q}_3 + 1.5\widehat{IQR} = & 59 + 37.5  = 96.5
\end{align*}

\vspace{0.3cm}

En nuestros datos, una tarea se realiz\'o en $139$ segundos, que est\'a  fuera del intervalo $[-3.5, 96.5]$. Este puede ser un valor at\'ipico.

\vspace{0.3cm}

Si la regla  $1.5(IQR)$  sugiere posibles valores at\'ipicos en la muestra a veces lo que se debe hacer algunas veces es suprimir las observaciones sospechosas, teniendo en cuenta que un valor at\'ipico puede afectar significativamente la media muestral  y la desviaci\'on est\'andar y por lo tanto echar a perder nuestro an\'alisis estad\'istico. Sin embargo, borrarlos de inmediato puede no ser la mejor idea, lo ideal es hacer un seguimiento  de los valores at\'ipicos y entender la raz\'on por la que han aparecido en el conjunto de datos.  A veces fen\'omenos importantes se descubren examinado valores at\'ipicos.


Si se confirma que una observaci\'on sospechosa entr\'o en el conjunto de datos por un simple error, se puede eliminar.

\vspace{0.8cm}

{\Large Gr\'aficos estad\'isticos}


\vspace{0.5cm}


\textbf{Histogramas}

\vspace{0.3cm}

Los histogramas muestran la forma de un \texttt{pmf} o \texttt{pdf} de la data, verifican la homogeneidad y sugieren posibles \texttt{outliers}. Para construir un histograma, dividimos el rango de los datos en iguales intervalos o \texttt{bins} y contamos las observaciones que pertenecen a cada intervalo.


\begin{itemize}
\item Un \texttt{histograma de frecuencia} consiste de columnas, una para cada intervalo, cuyo altura es determinada por el n\'umero de observaciones en el intervalo.
\item Un \texttt{histograma de frecuencias relativas} tiene la misma forma pero tiene una escala vertical diferente. La altura de las columnas representan la proporci\'on de todos los datos que aparecen en cada intervalo.

\end{itemize}

La muestra de los tiempos de demora de los CPU, cuenta desde los $9$ y $139$ segundos. Escogiendo los intervalos $[0, 14], [14, 28], [28, 42], \dots$, contamos

\begin{figure}[h]
\centering
\begin{BVerbatim}
 1 observacion   entre 0 y 14
 5 observaciones entre 14 y 28 
 9 observaciones entre 28 y 42
 7     "           "   42 y 56
 4     "           "   56 y 70
\end{BVerbatim}
\end{figure}


\vspace{0.5cm}

Usando esta informaci\'on para la altura de las columnas, un histograma de frecuencias de los tiempos del CPU pueden ser obtenido . Un histograma de frecuencias relativas es s\'olo diferente en la escala vertical, en el ejemplo  hemos dividido los valores de la escala vertical, entre el tama\~no muestral $n = 30$.

\vspace{0.3cm}

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{g4.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{g7.png}
  \end{minipage}
\end{figure}

\vspace{0.5cm}

La siguiente informaci\'on puede ser deducida por los histogramas de la figura anterior

\vspace{0.3cm}

\begin{itemize}
\item La  distribuci\'on  de los tiempos del CPU no es sim\'etrica; es sesgada a la derecha : 5 columnas a la derecha de la columna m\'as alta y s\'olo 2 columnas a la izquierda.
\item La distribuci\'on Gamma tiene una forma similar parece apropiada para los tiempos del CPU. \mbox{Esbozamos} el \texttt{pdf} de la distriuci\'o gamma adecuada reescalada porque las  columnas no tienen una unidad de ancho.
\item El tiempo $t=139$  se encuentra aislado y es por tanto  un outlier.
\item No hay ninguna indicaci\'on de  heterogeneidad; todos los datos excepto $x = 139$ forman un grupo bastante homog\'eneo que se ajuste a la curva gamma del gr\'afico.
\end{itemize}

\vspace{0.5cm}

Los histogramas vienen en todas las formas y tama\~nos

\vspace{0.3cm}

\begin{figure}[!hp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{g8.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{g9.png}
  \end{minipage}
\end{figure}

\vspace{0.3cm}

\begin{itemize}
\item En la primera figura, la distribuci\'on es casi sim\'etrica, y las columnas tienen casi la misma altura. Ligeras diferencias pueden atribuirse a la aleatoriedad de la muestra, es decir al error de muestreo. El histograma sugiere una distribuci\'on uniforme uniforme  entre a y b.
\item En la siguiente figura la distribuci\'on es muy sesgada a la  derecha, las alturas de las columnas disminuyen exponencialmente . Esta muestra debe venir de una distribuci\'on exponencial, si las variables son continuas, o de una distribuci\'on geométrica, si son discretas.
\end{itemize}

\newpage

En el siguiente ejemplo la distribuci\'on es sim\'etrica. Su forma es la densidad normal que decae con una taza de $\sim e^{-cx^2}$. Podemos localizar el centro de un histograma $\mu$ y concluir que esta muestra provenga de una distribuci\'on normal con una media cercana a $\mu$. 

\vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{g10.png}
\end{figure}

\vspace{0.3cm}


\textbf{Propiedad}: Los histogramas tienen una forma similar al \texttt{pmf} o \texttt{pdf} de los datos, especialmente en grandes muestras.

\vspace{0.8cm}

\textbf{Mezcla de distribuciones}

\vspace{0.3cm}

La mezcla de distribuciones aparecen en poblaciones heterog\'eneas que consisten de varios grupos: estudiantes graduados y no graduados, el tr\'afico de internet de dia o de noche, etc. En tales casos podemos estudiar cada grupo de forma separada, o usar la \texttt{la ley de probabilidad total} escribiendo el \texttt{cdf} como

\vspace{0.3cm}

\[
F(x) = p_1F_1(x) + p_2F_2(x) + \dots,
\]

\vspace{0.2cm}

y estudiar toda la poblaci\'on entonces.

\vspace{0.3cm}

La siguiente figura indica que la muestra proviene de una mezcla de dos distribuciones normales, con media $\mu_1$ y $\mu_2$ con una alta probabilidad de tener una media $\mu_1$, desde que la curva a la derecha es mayor:

\vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{g5.png}
\end{figure}

\vspace{0.5cm}

Estos  histogramas,  dependen de la elecci\'on de los intervalos. Este tipo de elecci\'on depende de ciertas reglas:

\begin{itemize}
\item No deber\'ian ser demasiados o pocos intervalos
\item Su n\'umero puede aumentar con el tama\~no de la muestra;
\item Deben ser elegidos de manera de  hacer el histograma muy informativo, como por ejemplo conocer  los outliers.
\end{itemize}

\vspace{0.5cm}



Consideramos los histogramas, construidos a partir de los mismos datos de los tiempo del CPU.

\begin{itemize}
\item El primer histograma tiene demasiadas columnas; por lo tanto, cada columna es corta. La mayor\'ia de los itervalos  s\'olo tienen 1 observaci\'on. Esto  dice poco acerca de la forma real de la distribuci\'on; sin embargo, todav\'ia podemos observar un outlier en $x = 139$.
\item El segundo histograma s\'olo tiene 3 columnas. Es dif\'icil saber  la familia de distribuciones aqu\'i. El outlier no es posible ser encontrado.
Ambos histogramas se pueden hacer m\'as informativos con  una mejor elecci\'on de intervalos.
\end{itemize}

\vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{g11.png}
\end{figure}
\textbf{Diagrama de cajas }

\vspace{0.3cm}

Las principales par\'ametros de estad\'stica descriptiva  de una muestra pueden ser representados gr\'aficamente mediante un diagrama de caja. Para construir un diagrama de cajas, se dibuja un cuadro entre el primer y el tercer cuartil, una l\'inea dentro de la caja para la  mediana, y se extiende los \texttt{whiskers (bigotes)} para los valores   m\'as peque\~nos y m\'as grandes de las  observaciones, lo cual se  resumen  de la siguiente manera:

\vspace{0.2cm}

\[
\texttt{resumen} = \Bigl(\min X_i,\ \hat{Q_1},\  \hat{M},\  \hat{Q_3},\  \max X_i \Bigr).
\]

\vspace{0.2cm}

La media $\overline{X}$ es a menudo representada por puntos o cruces. Observaciones m\'as all\'a de que el rango  $1.5$ intercuartil son usualmente separados de los whiskers, indicando la posibilidad de outliers. Esto de es de acuerdo a la regla $1.5(IQR)$.

\vspace{0.5cm}

La media y el resumen para los tiempos de uso del CPU, en los ejemplos anteriores, producen: 

\vspace{0.2cm}

$\overline{X} = 48.2333, \min X_i = 9,  \hat{Q_1} = 34,   \hat{M} = 42.5, \hat{Q_3} = 59, \max X_i = 139$

\vspace{0.2cm}


Se sabe adem\'as que $X = 139$ es m\'as que $1.5(\widehat{IQR})$ de la distancia al tercer cuartil, lo que invita a que es un outlier.


\vspace{0.3cm}

\begin{figure}[h]
\centering
\includegraphics[scale=.5]{g6.png}
\end{figure}

\vspace{0.3cm}

Un diagrama de cajas es mostrado en la figura anterior. La media es representada por un \texttt{'+'} el whiskers de la derecha se extiende hasta la segunda mayor observaci\'on $X = 89$, ya que $X = 139$, se espera ser un outlier (representado por el peque\~no c\'irculo).

Desde este diagrama de cajas, uno puede concluir que:

\begin{itemize}
\item La distribuci\'on de los tiempos del CPU es sesgada a la derecha debido a que la media supera a la mediana, y el lado derecho  de la caja es mayor que el lado  izquierdo.
\item Cada mitad  de una caja y cada whiskers representa aproximadamente el $25\%$ de la poblaci\'on. Por ejemplo, se espera que aproximadamente el $25\%$ de todos los tiempos del  CPU  est\'en ente $42,5$ y $59$ segundos.
\end{itemize}


\vspace{0.5cm}


\textbf{Diagrama de cajas paralelas}

\vspace{0.3cm}

Los diagramas de caja a menudo se utilizan para comparar diferentes poblaciones o partes de la misma poblaci\'on. Para una comparaci\'on de este tipo, los datos se recogen de cada parte, y sus diagramas de caja se dibujan en la misma escala junto a la otra.

Por ejemplo, el siguiente gr\'afico presenta  siete diagramas de cajas que representan el  tr\'afico de internet  durante una semana.

\vspace{0.3cm}


\begin{figure}[h]
\centering
\includegraphics[scale=.5]{g12.png}
\end{figure}

\vspace{0.3cm}

Podemos ver de este gr\'afico las siguientes caracter\'isticas:

\vspace{0.2cm}

\begin{itemize}
\item La mayor tr\'afico de Internet se produce los viernes.
\item Los viernes tambi\'en tiene mayor variabilidad.
\item El tr\'afico m\'as ligero se ve en los fines de semana, con una tendencia creciente desde el s\'abado al lunes.
\item Cada d\'ia, la distribuci\'on es sesgada a la derecha, con unos pocos outliers en cada d\'ia excepto los s\'abados. Los outliers indican que el tr\'afico de Internet es inusualmente pesado .
\end{itemize}


\newpage


\textbf{Gr\'aficos de dispersi\'on}

\vspace{0.3cm}

Los diagramas de dispersi\'on se utilizan para ver y analizar la  relaci\'on entre dos variables. Por ejemplo podemos tener la relaci\'on entre la  temperatura y la humedad, la experiencia y el salario, el tiempo de una red y su velocidad, el n\'umero de servidores y el tiempo de respuesta , y as\'i sucesivamente.

Para estudiar la relaci\'on entre ambas variables se mide  cada elemento de la muestra. Por ejemplo, la temperatura y la humedad durante $n$ d\'ias, el tiempo y la velocidad de  $n$ redes,  la experiencia y el salario de $n$ matem\'aticos elegidos al azar. Entonces, un \texttt{diagrama de dispersi\'on} consiste de $n$ puntos sobre el plano $(x,y)$, con $x$ e $y$, que representa las coordenadas de las dos variables registradas.

\vspace{0.5cm}

\textbf{Ejemplo} Durante un mantenimiento programado, un servidor  registra el n\'umero de veces que un  antivirus se puso en marcha en cada computador durante 1 mes (variable X) y el n\'umero de troyanos  detectados (variable Y). Los datos para 30 computadoras est\'an en la tabla.

\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{c|ccccccccccccccc}
X & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 30 & 15 & 15 & 15 & 10 \\
\hline
Y & 0  & 0  & 1  & 0  & 0  & 0  & 1  & 1  & 0  & 0  & 0  & 0  & 1  & 1  & 0 
\end{tabular}
\end{table}

\vspace{0.3cm}

\begin{table}[h]
\centering
\begin{tabular}{c|ccccccccccccccc}
X & 10 & 10 & 6 & 6 & 5 & 5 & 5 & 4 & 4 & 4 & 4 & 4 & 1 & 1 & 1 \\
\hline
Y &  0  &  2  &  0  &  4  &  1  & 2  & 0  & 2  & 1  & 0  & 1  & 0  & 6  & 3  & 1 
\end{tabular}
\end{table}

\vspace{0.3cm}


Dos gr\'afico de dispersi\'on de estos datos se muestra a continuaci\'on

\vspace{0.3cm}

\begin{figure}[!hp]
  \centering
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{g14.png}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{g15.png}
  \end{minipage}
\end{figure}


\vspace{0.3cm}


En el primer gr\'afico el n\'umero de troyanos se  reduce,  cuando el antivirus se emplea con mayor frecuencia. Esta relaci\'on, sin embargo, no es segura porque ning\'un troyano fue detectado en algunas computadoras 'afortunadas' aunque el  antivirus se puso en marcha una vez por semana en esas computadoras. Aademás que no se detectaron troyanos  en 8 equipos en los que se usa el  antivirus todos los dias. La figura puede ser enga\~nosa

Cuando los datos contienen pares  de observaciones, los puntos en un diagrama  de dispersi\'on se representan a menudo con n\'umeros o letras ('A' por 1 punto 'B' por dos puntos id\'enticos, 'C' para tres, etc.), como se muestra en la segunda figura.
\end{document}
