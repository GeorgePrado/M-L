\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{bigints}
\usepackage[]{algorithm2e}
\geometry{verbose,tmargin=1.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\begin{document}
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(grid)
# set global chunk options
opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE,width=90)
@


\title{Lista de ejercicios 4}


\author{Curso: T\'opicos de Investigaci\'on : Machine Learning CM-072}
\date{}
\maketitle

\vspace{0.3cm}


\textbf{Lecturas Importantes }
\begin{enumerate}
\item  Introducci\'on al Machine Learning usando R:

\url{www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/}.
\end{enumerate}
%{\normalsize Los c\'odigos, se presentaran impresos,  o como un archivo con extensi\'on $.R$, mostrando ejemplos de su ejecuci\'on.}
\setlength{\unitlength}{1in}

\begin{picture}(6,.1) 
\put(0,0) {\line(1,0){6.25}}         
\end{picture}

\vspace{0.5cm}


\begin{enumerate}
\item Sup\'ongase que se nos da la tarea de construir un sistema para distinguir  correo basura.
\textquestiondown C\'omo se  puede detectar en el  equipo correo  no deseado a trav\'es de un an\'alisis sint\'actico?. \textquestiondown Qu\'e nos gustar\'ia que el equipo haga si detecta un correo electr\'onico no deseado, que lo  borre de forma autom\'atica, lo mueva a un archivo diferente, o simplemente lo resalte  en la pantalla ?.
\item Digamos que se nos da la tarea de construir un taxi autom\'atico. Define las restricciones. \textquestiondown Cu\'ales son las entradas? \textquestiondown Cu\'al es la salida? \textquestiondown C\'omo podemos comunicarnos con el pasajero? \textquestiondown Es necesario  comunicarse con los otros taxis autom\'aticos, es decir,  es necesario  un \texttt{lenguaje}?.
\item Si una imagen facial  es una imagen de $100 \times 100$ , es decir   un vector de  dimensi\'on $10 000$. Si desplazamos la imagen un p\'ixel a la derecha, tenemos  un vector muy diferente en el espacio de $10 000$ dimensiones. \textquestiondown C\'omo podemos construir reconocedores faciales   a tales distorsiones?.
\item En un diario, encontramos  cinco muestras de  informes de noticias  para cada categor\'ia como \texttt{pol\'itica}, \texttt{deportes} y la \texttt{artes}. Si queremos  encontrar palabras sobre estas muestras  que se utilizan con frecuencia para cada categor\'ia, las cuales pueden ayudarnos a discriminar  entre las diferentes categor\'ias. Por ejemplo, en un informe de noticias sobre pol\'itica es probable  que se incluyen palabras como \texttt{gobierno}, \texttt{recesi\'on}, \texttt{Congreso}, y as\'i sucesivamente, mientras que un informe de noticias sobre  \texttt{artes} puede incluir \texttt{disco},  \texttt{lienzo}, o \texttt{teatro }. tambi\'en hay palabras tales como \texttt{objetivo} que son ambiguas. \textquestiondown C\'omo proceder en esos casos?.
\item  Una moneda es lanzada $n$ veces, mostrando caras $H_n$ veces y sellos $T_n$ veces. Sea $S_n = H_n - T_n$. Muestra que 

\[
\mathbb{P}(S_n > an)^{1/n} \rightarrow \dfrac{1}{\sqrt{(1 +a)^{1 +a}(1 -a)^{1 -a}}}\ \ \text{si}\ \ 0 < a < 1.
\]

?` Qu\'e ocurre si $a \geq 1$?.
\item  Sean $X$ y $Y$ variables aleatorias independientes  con funci\'on densidad com\'un $f(x) =1$ si $0 < x  < 1$, $f(x) = 0$ en otros casos.

Sea $U = I_{\{Y \leq g(X)\}}$, la funci\'on indicador del evento que $Y \leq g(X)$ y sea $V = g(X)$, $W(X) = \frac{1}{2}\{g(X) +g(1 -X)\}$. 

Muestra que $\mathbb{E}(U) = \mathbb{E}(V) = \mathbb{E}(W) = J$ y que $\mathbb{V}(W) \leq \mathbb{V}(V) \leq \mathbb{V}(U)$. Prueba que $W$ es el estimador m\'as eficiente de $J$.
\item Supongamos que tenemos una matriz cuadrada $M \in \mathbb{R}^{n \times n}$ es escrita en forma de bloques

\[
M = \begin{pmatrix}
A & B \\
C & D
\end{pmatrix}
\]

donde $A \in \mathbb{R}^{ k\times k}$.

\begin{itemize}
	\item Muestra que podemos descomponer $M$ como el producto
	
	\[
	M = \begin{pmatrix}
	I & O \\
	CA^{-1} & I
	\end{pmatrix} 
	\begin{pmatrix}
	A & O \\
	0 & D - CA^{-1}B
	\end{pmatrix} 
	\begin{pmatrix}
	I & A^{-1}B \\
	O & I
	\end{pmatrix}
	\]
	donde $I$ es la matriz identidad de tama\~no apropiado.
	\item Supongamos que descomponemos $A = L_1U_1$ y $D -CA^{-1}B = L_2U_2$. Muestra como construir una factorizaci\'on $LU$ de $M$ dados esas matrices adicionales.
	\item Usa esta estructura para definir un algoritmo recursivo para la factorizaci\'on $LU$, se puede asumir que $n = 2^{l}$ para alg\'un $l >0$.
\end{itemize}
\item  Una versi\'on general de la descomposici\'on de Cholesky, que no requiere el c\'alculo de las raices es la \texttt{descomposici\'on LLT}

\begin{itemize}
	\item Supongamos que $A \in \mathbb{R}^{n \times n}$ es sim\'etrica y admite una factorizaci\'on $LU$ (sin pivot). Muestra que $A$ puede ser factorizada como $A = LDL^{T}$, donde $D$ es  diagonal y $L$ es una matriz triangular.
	\item Modifica la construcci\'on de la descomposici\'on de Cholesky para mostrar que una matriz sim\'etrica, definida positiva $A$ puede ser factorizada como $A = LDL^{T}$ sin usar alguna operaci\'on de raices cuadradas.  
\end{itemize}
\item Explica y corrige  el siguiente c\'odigo que implementa el \texttt{perceptron}

\begin{verbatim}
import numpy as np

class Perceptron(object):

   def __init__(self, eta=0.01, n_iter=10):
      self.eta = eta
      self.n_iter = n_iter

   def fit(self, X, y):
      self.w_ = np.zeros(1 + X.shape[1])
      self.errors_ = []
  
      for _ in range(self.n_iter):
         errors = 0
	     for xi, target in zip(X, y):
		     update = self.eta * (target - self.predict(xi))
             self.w_[1:] += update * xi
             self.w_[0] += update
             errors += int(update != 0.0)
         self.errors_.append(errors)
       return self
  
   def net_input(self, X):
      return np.dot(X, self.w_[1:]) + self.w_[0]
   
   def predict(self, X):
      return np.where(self.net_input(X) >= 0.0, 1, -1)
\end{verbatim}
\end{enumerate}
\end{document}